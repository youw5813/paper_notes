# Context Engineering
## Tweets

[Tobi Lutke's Tweet](https://x.com/tobi/status/1935533422589399127)
>I really like the term “context engineering” over prompt engineering. It describes the core skill better: the art of providing all the context for the task to be plausibly solvable by the LLM.

[Andrej Karpathy's Tweet](https://x.com/karpathy/status/1937902205765607626)
>(Context engineering is the) …delicate art and science of filling the context window with just the right information for the next step.
## Blogs

### What
- [The New Skill in AI is Not Prompting, It's Context Engineering](https://www.philschmid.de/context-engineering)
	- Context is not just a single prompt sent to LLM. It's everything the model sees before it generates a response. That includes system prompt, user prompt, short-term memory, long-term memory, RAG modules, tools, structured output, etc.
	- A lot of agent failures nowadays is not model failures, it's context failures. i.e. the LLM didn't get sufficient context to accomplish the task. 
### Why
- [How Long Contexts Fail \| Drew Breunig](https://www.dbreunig.com/2025/06/22/how-contexts-fail-and-how-to-fix-them.html) \*\*
	- **Summary**: Context window isn't the longer the better. Overly long context could bring new problems. Smaller models are more prone to context problems compared with larger models. 
	- **Context Poisoning**: Error information in the context. This could be hallucination generated by itself and referenced repeatedly.
	- **Context Distraction**: Model over-focusing on context and ignoring knowledge learnt from training data when the context is too long. 
	- **Context Confusion**: Model paying attention to superfluous tools/information that is irrelevant to the task. 
	- **Context Clash**: New information and tools conflicting with existing information in the context. 
		- A research by Microsoft and Salesforce team showed that model performs better with 1 prompt containing all the useful information than with multiple prompts containing complementary information. This is because model tend to make assumptions when there's insufficient information. These wrong early answers will continue existing in the context and affect the model's self-correcting ability. 
### How
- [How to Fix Your Context \| Drew Breunig](https://www.dbreunig.com/2025/06/26/how-to-fix-your-context.html)
	- RAG
	- Tool Loadout: carefully selecting the tools to provide to LLM
	- Context Quarantine: isolating contexts in their own dedicated threads, each used separately by one or more LLMs.
	- Context Pruning: removing the irrelevant information in the context
	- Context Summarization
	- Context Offloading: storing the additional information outside of the LLM's context via a tool that stores and manages the data. Anthropic found that it helps most in the following three scenarios,
		- tool output analysis - before acting on a tool call
		- policy-heavy environment
		- sequential decision making where each action depends on the previous ones
- [Lance's Blog: Context Engineering for Agents](https://rlancemartin.github.io/2025/06/23/context_engineering/) \*\*\*
	- Context types
		- Instructions – prompts, memories, few‑shot examples, tool descriptions, etc
		- Knowledge – facts, memories, etc
		- Tools – feedback from tool calls
	- Write Context: Save important information outside of context window so that it won't be lost and can be accessible to the agent. 
		- scratchpads: calling a tool, a state object within a session
		- memory: usually across sessions, self-generated memories
	- Select Context: select relevant context and pull it into the context window to help an agent with the task. Embeddings and knowledge graphs can be used to assist with context selection. 
		- scratchpad: the developer could choose how to expose the context
		- memories: ensure the relevant memories are selected
			- semantic: facts - about a user
			- episodic: experience - previous agent actions
			- procedural: instructions - system prompts
		- tools: agents might get overwhelmed if provided with too many tools. [[Retrieval Augmented Generation|RAG]] is good way to improve this. 
		- knowledge: Simple retrieving methods will not always work as the global context grows. A better system would utilise a combination or file search, [[Retrieval Augmented Generation|RAG]], knowledge graph, and some reranking mechanism to select the context in an optimal way.
	- Compress Context: retaining only the token relevant to solve the tasks
		- context summarisation
			- [hierarchical summarisation](https://alignment.anthropic.com/2025/summarization-for-monitoring/#:~:text=We%20addressed%20these%20issues%20by,of%20our%20computer%20use%20capability)
			- [recursive summarisation](https://arxiv.org/pdf/2308.15022#:~:text=the%20retrieved%20utterances%20capture%20the,based%203)
		- context trimming
			- [pruning](https://www.dbreunig.com/2025/06/26/how-to-fix-your-context.html)
	- Isolate Context: split the context up to help an agent to perform a task
		- multi-agent: Each agent is exposed to a part of the context that is helpful for the subtask that the agent needs to accomplish.
		- state: A state object contains a scheme which is a TypedDict or a Pydantic model. The fields in the schema can be used to store context and a selective set of fields can be exposed to LLM where necessary. 
- [Context Engineering - What it is, and techniques to consider — LlamaIndex - Build Knowledge Assistants over your Enterprise Data](https://www.llamaindex.ai/blog/context-engineering-what-it-is-and-techniques-to-consider)
	- Knowledge of available knowledge base and tools
	- Context ordering and compression. Order can be important in some tasks for example tasks require date information.
	- Long-term memory storage and retrieval. Long-term memory can be stored in different format (vector, facts, static memory, etc...). The decision of which memories to retrieve is important.
	- Structured information can be used to make the context shorter.
	- Workflow engineering is important as it could prevent context overload by dividing tasks into subtasks, each with its own optimised context window.
- [Cognition \| Don’t Build Multi-Agents](https://cognition.ai/blog/dont-build-multi-agents#a-theory-of-building-long-running-agents) \*\*
	- Multi-agent systems where multiple agents working on different subtasks in parallel fails due to mismatch in communication and lack of global context. 
	- Single-threaded agent is more reliable but suffer from context overflow issues. 
	- One solution is to use context compression on single-thread agent - summarising the context from previous action before adding to the context for the next action. 
	- They propose 2 principle in building an agentic system
		- Share context
		- Actions carry implicit decisions
